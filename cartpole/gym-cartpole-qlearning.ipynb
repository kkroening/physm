{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from ipywidgets import interact\n",
    "from IPython.display import display\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from xvfbwrapper import Xvfb\n",
    "import atexit\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import PIL.Image\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if globals().get('virtual_display') is None or 'DISPLAY' not in os.environ:\n",
    "    try:\n",
    "        virtual_display = Xvfb()\n",
    "        virtual_display.start()\n",
    "    except:\n",
    "        virtual_display = None\n",
    "    atexit.register(virtual_display.stop)\n",
    "    print('Started xvfb: DISPLAY={!r}'.format(os.environ['DISPLAY']))\n",
    "else:\n",
    "    print('Using DISPLAY={!r}'.format(os.environ['DISPLAY']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_np_image(np_image):\n",
    "    display(PIL.Image.fromarray(np_image, 'RGB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "LEARNING_RATE = 0.001\n",
    "MEMORY_SIZE = 1000000\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_RUNS = 1\n",
    "MAX_TIME = 200\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.998\n",
    "\n",
    "\n",
    "def select_action(model, state, exploration_rate):\n",
    "    if np.random.rand() < exploration_rate:\n",
    "        action = random.randrange(N_ACTIONS)\n",
    "    else:\n",
    "        q_values = model.predict(state[np.newaxis,:])\n",
    "        #print(q_values)\n",
    "        action = np.argmax(q_values[0])\n",
    "    return action\n",
    "\n",
    "\n",
    "def get_updated_q_value(action, reward, done, q_value1, q_value2):\n",
    "    if done:\n",
    "        q_update = -10\n",
    "    else:\n",
    "        q_update = reward + GAMMA * np.argmax(q_value2)\n",
    "    new_q_value = q_value1.copy()\n",
    "    new_q_value[action] = q_update\n",
    "    return new_q_value\n",
    "\n",
    "\n",
    "def replay_memories(model, memories, exploration_rate):\n",
    "    if len(memories) >= BATCH_SIZE:\n",
    "        memory_batch = random.sample(memories, BATCH_SIZE)\n",
    "        states = [x['state'] for x in memory_batch]\n",
    "        next_states = [x['next_state'] for x in memory_batch]\n",
    "        predictions = model.predict(np.vstack((states, next_states)))\n",
    "        q_values1 = predictions[:len(states)]\n",
    "        q_values2 = predictions[len(states):]\n",
    "        new_q_values = []\n",
    "        for memory, q_value1, q_value2 in zip(memory_batch, q_values1, q_values2):\n",
    "            action = memory['action']\n",
    "            reward = memory['reward']\n",
    "            done = memory['done']\n",
    "            new_q_values += [get_updated_q_value(action, reward, done, q_value1, q_value2)]\n",
    "        model.fit(np.array(states), np.array(new_q_values), verbose=0)\n",
    "        exploration_rate *= EXPLORATION_DECAY\n",
    "        exploration_rate = max(EXPLORATION_MIN, exploration_rate)\n",
    "    return memories, exploration_rate\n",
    "\n",
    "\n",
    "def run(env, model, memories, exploration_rate):\n",
    "    state = env.reset()\n",
    "    states = [state]\n",
    "    rewards = [None]\n",
    "    dones = [False]\n",
    "    actions = []\n",
    "    for i in tqdm(range(MAX_TIME)):\n",
    "        action = select_action(model, state, exploration_rate)\n",
    "        actions.append(action)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        memories.append({\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'next_state': next_state,\n",
    "            'done': done\n",
    "        })\n",
    "        state = next_state\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        if done:\n",
    "            break\n",
    "        memories, exploration_rate = replay_memories(model, memories, exploration_rate)\n",
    "    actions.append(None)\n",
    "    return states, actions, rewards, dones, memories, exploration_rate\n",
    "\n",
    "\n",
    "def show_scores(states, score_smoothing=20):\n",
    "    scores = [len(x) for x in states]\n",
    "    scores_smoothed = np.convolve(scores, np.ones((score_smoothing,)) / score_smoothing, mode='valid')\n",
    "    plt.plot(scores_smoothed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(20, input_shape=(env.observation_space.shape[0],), activation='relu'),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dense(env.action_space.n, activation='linear'),\n",
    "])\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "memories = deque(maxlen=MEMORY_SIZE)\n",
    "exploration_rate = EXPLORATION_MAX\n",
    "states = []\n",
    "actions = []\n",
    "rewards = []\n",
    "dones = []\n",
    "run_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "    print(f'Trial {run_count}   (exploration rate: {exploration_rate})')\n",
    "    (\n",
    "        more_states,\n",
    "        more_actions,\n",
    "        more_rewards,\n",
    "        more_dones,\n",
    "        memories,\n",
    "        exploration_rate\n",
    "    ) = run(env, model, memories, exploration_rate)\n",
    "    print(more_actions)\n",
    "    states += [more_states]\n",
    "    actions += [more_actions]\n",
    "    rewards += [more_rewards]\n",
    "    dones += [more_dones]\n",
    "    run_count += 1\n",
    "\n",
    "show_scores(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_scores(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(state):\n",
    "    env.env.state = state\n",
    "    return env.render(mode='rgb_array')\n",
    "\n",
    "\n",
    "@interact(\n",
    "    trial=(0, len(states) - 1),\n",
    "    time=(0, MAX_TIME - 1),\n",
    ")\n",
    "def f(trial=1105, time=0):\n",
    "    if time < len(states[trial]):\n",
    "        display_np_image(render(states[trial][time]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_renderer_widget import Renderer\n",
    "\n",
    "trial = 980\n",
    "trial = min(len(states) - 1, trial)\n",
    "trial_states = states[trial]\n",
    "display(Renderer(lambda t: render(trial_states[t]), len(trial_states) - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    observations = [env.reset()]\n",
    "    rewards = [None]\n",
    "    dones = [False]\n",
    "    for i in range(100):\n",
    "        action = i % 2  # FIXME\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        observations.append(observation)\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    with open('out.json', 'w') as f:\n",
    "        f.write(json.dumps({\n",
    "            'actions': actions,\n",
    "            'rewards': rewards,\n",
    "            'states': states,\n",
    "\n",
    "        }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for trial in range(len(states)):\n",
    "    for step in range(len(states[trial])):\n",
    "        step_state = states[trial][step]\n",
    "        row = {\n",
    "            'trial': trial,\n",
    "            'step': step,\n",
    "            'reward': rewards[trial][step],\n",
    "            'done': dones[trial][step],\n",
    "        }\n",
    "        for k, state in enumerate(states[trial][step]):\n",
    "            row[f'observation{k}'] = state\n",
    "        rows.append(row)\n",
    "pd.DataFrame(rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
