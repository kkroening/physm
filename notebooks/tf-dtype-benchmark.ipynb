{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['XLA_FLAGS'] = '--xla_dump_hlo_as_text --xla_dump_to=/home/karl/tmp/hlo'\n",
    "import tensorflow as tf\n",
    "tf.config.optimizer.set_jit(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "from datetime import datetime\n",
    "from timeit import timeit\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(dtype):\n",
    "    cast = lambda x: tf.cast(tf.floor(x), dtype)\n",
    "    a = cast(tf.random.uniform((1000, 1000)) * 100)\n",
    "    b = cast(tf.random.uniform((1000,)))\n",
    "    return a, b\n",
    "\n",
    "\n",
    "def py_func(a, b):\n",
    "    c = tf.zeros_like(a)\n",
    "    for d in b:\n",
    "        c += d * a\n",
    "    return c\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def tf_func(a, b):\n",
    "    return py_func(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def benchmark(key, stats={}):\n",
    "    @contextmanager\n",
    "    def trial():\n",
    "        start = datetime.now()\n",
    "        yield stat\n",
    "        end = datetime.now()\n",
    "        duration = (end - start).total_seconds()\n",
    "        stat.append(duration)\n",
    "        print('  trial {}: {:.6f}'.format(len(stat), stat[-1]))\n",
    "\n",
    "    stat = []\n",
    "    print(f'{key}:')\n",
    "    stats[key] = stat\n",
    "    yield trial\n",
    "    if not stat:\n",
    "        print('  (no trials)')\n",
    "    else:\n",
    "        mean = sum(stat) / len(stat)\n",
    "        deviation = (sum([(x - mean) ** 2 for x in stat]) / (len(stat) + 1)) ** 0.5\n",
    "        print('  mean: {:.6f}'.format(mean))\n",
    "        print('  dev: {:.6f}'.format(deviation))\n",
    "        print('  min: {:.6f}'.format(min(stat)))\n",
    "        print('  max: {:.6f}'.format(max(stat)))\n",
    "\n",
    "\n",
    "def do_benchmark(func, dtype, iterations=30):\n",
    "    a, b = prepare(dtype)\n",
    "    key = f'{func.__name__} {dtype.name}'\n",
    "    with benchmark(key, stats) as trial:\n",
    "        for i in range(iterations):\n",
    "            with trial() as stat:\n",
    "                func(a, b)\n",
    "            if stat[-1] > 3 and i > 5:\n",
    "                break\n",
    "\n",
    "\n",
    "stats = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes = [\n",
    "    tf.float32,\n",
    "    tf.float64,\n",
    "    tf.complex64,\n",
    "    tf.complex128,\n",
    "    tf.int8,\n",
    "    tf.int16,\n",
    "    tf.int32,\n",
    "    tf.uint8,\n",
    "]\n",
    "funcs = [py_func, tf_func]\n",
    "for dtype, func in itertools.product(dtypes, funcs):\n",
    "    do_benchmark(func, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('tf-dtype-benchmark-xla.json', 'w') as f:\n",
    "    f.write(json.dumps(stats, indent=2, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
